{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "xlm_model",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "151057b3bfb244a49e0398f55724df3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9f41dd6aa88d403a8f656477b3244604",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_985912cd05ba4e1b9460ee2cb31a7c96",
              "IPY_MODEL_64ce39d44db641e78d91770c6de2083b"
            ]
          }
        },
        "9f41dd6aa88d403a8f656477b3244604": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "985912cd05ba4e1b9460ee2cb31a7c96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1aabebc1ace14f8da957a123e6a2319b",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 5600090,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 5600090,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_20bfdff5f83a42df80a4da2792f8d0f6"
          }
        },
        "64ce39d44db641e78d91770c6de2083b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_743ccd5a7367485a84f00ccd5d0de389",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5.60M/5.60M [00:01&lt;00:00, 3.40MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_194e68dcaa5f4b8292e9a7eb34eb8028"
          }
        },
        "1aabebc1ace14f8da957a123e6a2319b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "20bfdff5f83a42df80a4da2792f8d0f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "743ccd5a7367485a84f00ccd5d0de389": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "194e68dcaa5f4b8292e9a7eb34eb8028": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "161425b44b164dd9a399d318e36e91ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_54405d2432064e25b6eb299bc119a3bc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3b81c961288f437195fefc47deccf1e0",
              "IPY_MODEL_3a0080a1b3054833a342323e58653148"
            ]
          }
        },
        "54405d2432064e25b6eb299bc119a3bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3b81c961288f437195fefc47deccf1e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7a9ba484886746ac80e7bfbc997c11af",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2986457,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2986457,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4af3be6526af4c0db5bb8989ebb3a46a"
          }
        },
        "3a0080a1b3054833a342323e58653148": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2423dd243d644bdab1a52a07fa2795b1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2.99M/2.99M [00:00&lt;00:00, 4.62MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_04b29f18afef41259e871831abb28a7a"
          }
        },
        "7a9ba484886746ac80e7bfbc997c11af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4af3be6526af4c0db5bb8989ebb3a46a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2423dd243d644bdab1a52a07fa2795b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "04b29f18afef41259e871831abb28a7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dd55c54c6ae34699a44b8411df42202f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_37cbcd897503496bbb6a43ba65055d52",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_71b07e89dad846ba935bf63a730bf2d4",
              "IPY_MODEL_f6a1b2f4fe32453799d4f8165ebe805d"
            ]
          }
        },
        "37cbcd897503496bbb6a43ba65055d52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "71b07e89dad846ba935bf63a730bf2d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_fb7484b6ad61480f83d0cb16530f0ea8",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 10788,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 10788,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_06b3e73098c347e58b29d2567d1e89c3"
          }
        },
        "f6a1b2f4fe32453799d4f8165ebe805d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d65426c1d23e4c9dbd855551a2101acc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 10.8k/10.8k [00:00&lt;00:00, 24.8kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1288cc6dec804032b3f7b2cfa533d4ac"
          }
        },
        "fb7484b6ad61480f83d0cb16530f0ea8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "06b3e73098c347e58b29d2567d1e89c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d65426c1d23e4c9dbd855551a2101acc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1288cc6dec804032b3f7b2cfa533d4ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f456e5594df441bf91a68aea64f49e9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9c394538a74a413b9defd99d9bbf9931",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_af768710b35a401490074940db3d14eb",
              "IPY_MODEL_bc82ca7a2c604213b30e7960c6500784"
            ]
          }
        },
        "9c394538a74a413b9defd99d9bbf9931": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "af768710b35a401490074940db3d14eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_42762f90f42d401280d2333f4a9b927d",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1143436263,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1143436263,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d99a8f49d5884aedb8128a6884bd4199"
          }
        },
        "bc82ca7a2c604213b30e7960c6500784": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7ce05cd039164962b6e714921139dff4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.14G/1.14G [00:32&lt;00:00, 35.1MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_212cdd6560e149988bafd2ae0a74c465"
          }
        },
        "42762f90f42d401280d2333f4a9b927d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d99a8f49d5884aedb8128a6884bd4199": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7ce05cd039164962b6e714921139dff4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "212cdd6560e149988bafd2ae0a74c465": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L847vafDKQ1V",
        "outputId": "1686809c-ae2f-4891-d93d-28f2386c3138"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Dec 23 03:59:04 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.27.04    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   71C    P0    32W /  70W |  14717MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7xvjUpZh2Ii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47556d70-6cd0-4a83-9dd6-295b2cbbb48c"
      },
      "source": [
        "import re\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import numpy as np      \n",
        "import pandas as pd       \n",
        "import matplotlib.pyplot as plt   \n",
        "import seaborn as sns\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import DataLoader\n",
        "import gc\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():  \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "    \n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "    \n",
        "pd.options.display.max_colwidth = None\n",
        "pd.options.display.max_rows = 100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9Irm4p3iI68",
        "outputId": "84dd3484-2821-4fe1-eaa9-3788eced5853"
      },
      "source": [
        "train = pd.read_csv(\"/content/drive/My Drive/dacon/news_train.csv\") # train.csv 불러오기\n",
        "test = pd.read_csv(\"/content/drive/My Drive/dacon/news_test.csv\") # test.csv 불러오기\n",
        "train[\"id\"] = train[\"n_id\"].astype(str) + '_' + train[\"ord\"].astype(str)\n",
        "train.drop(['n_id', 'ord'],axis = 1) \n",
        "train['length'] = train['content'].apply(lambda x: len(x))\n",
        "print(test.shape)\n",
        "print(train.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(142565, 6)\n",
            "(118745, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWIA-pxkb0KK"
      },
      "source": [
        "def preprocess(text):\n",
        "    # remove hyperlinks\n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n",
        "    text = re.sub(r'http?:\\/\\/.*[\\r\\n]*', '', text)\n",
        "    #remove some puncts (except . ! ?)\n",
        "    text=re.sub(r'[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', '',text)\n",
        "    text=\" \".join(text.split())\n",
        "    return text\n",
        "\n",
        "train['clean_text'] = train['content'].apply(preprocess)\n",
        "test['clean_text'] = test['content'].apply(preprocess)\n",
        "train['clean_title'] = train['title'].apply(preprocess)\n",
        "test['clean_title'] = test['title'].apply(preprocess)\n",
        "\n",
        "test.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CU3SrM7VIClo"
      },
      "source": [
        "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=2, warmup_steps=50, evaluator=evaluator2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "151057b3bfb244a49e0398f55724df3c",
            "9f41dd6aa88d403a8f656477b3244604",
            "985912cd05ba4e1b9460ee2cb31a7c96",
            "64ce39d44db641e78d91770c6de2083b",
            "1aabebc1ace14f8da957a123e6a2319b",
            "20bfdff5f83a42df80a4da2792f8d0f6",
            "743ccd5a7367485a84f00ccd5d0de389",
            "194e68dcaa5f4b8292e9a7eb34eb8028",
            "161425b44b164dd9a399d318e36e91ed",
            "54405d2432064e25b6eb299bc119a3bc",
            "3b81c961288f437195fefc47deccf1e0",
            "3a0080a1b3054833a342323e58653148",
            "7a9ba484886746ac80e7bfbc997c11af",
            "4af3be6526af4c0db5bb8989ebb3a46a",
            "2423dd243d644bdab1a52a07fa2795b1",
            "04b29f18afef41259e871831abb28a7a",
            "dd55c54c6ae34699a44b8411df42202f",
            "37cbcd897503496bbb6a43ba65055d52",
            "71b07e89dad846ba935bf63a730bf2d4",
            "f6a1b2f4fe32453799d4f8165ebe805d",
            "fb7484b6ad61480f83d0cb16530f0ea8",
            "06b3e73098c347e58b29d2567d1e89c3",
            "d65426c1d23e4c9dbd855551a2101acc",
            "1288cc6dec804032b3f7b2cfa533d4ac",
            "f456e5594df441bf91a68aea64f49e9a",
            "9c394538a74a413b9defd99d9bbf9931",
            "af768710b35a401490074940db3d14eb",
            "bc82ca7a2c604213b30e7960c6500784",
            "42762f90f42d401280d2333f4a9b927d",
            "d99a8f49d5884aedb8128a6884bd4199",
            "7ce05cd039164962b6e714921139dff4",
            "212cdd6560e149988bafd2ae0a74c465"
          ]
        },
        "id": "vLwA2oAVhtH1",
        "outputId": "f087ea4f-cd56-4567-d88d-2544c85ba8a0"
      },
      "source": [
        "from transformers import XLMTokenizer, XLMModel, XLMForSequenceClassification\n",
        "\n",
        "tokenizer = XLMTokenizer.from_pretrained(\"xlm-mlm-17-1280\")\n",
        "model = XLMForSequenceClassification.from_pretrained(\"xlm-mlm-17-1280\",num_labels=2)\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "151057b3bfb244a49e0398f55724df3c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=5600090.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "161425b44b164dd9a399d318e36e91ed",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2986457.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd55c54c6ae34699a44b8411df42202f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=10788.0, style=ProgressStyle(descriptio…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f456e5594df441bf91a68aea64f49e9a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1143436263.0, style=ProgressStyle(descr…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at xlm-mlm-17-1280 were not used when initializing XLMForSequenceClassification: ['pred_layer.proj.weight', 'pred_layer.proj.bias']\n",
            "- This IS expected if you are initializing XLMForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLMForSequenceClassification were not initialized from the model checkpoint at xlm-mlm-17-1280 and are newly initialized: ['transformer.position_ids', 'sequence_summary.summary.weight', 'sequence_summary.summary.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XLMForSequenceClassification(\n",
              "  (transformer): XLMModel(\n",
              "    (position_embeddings): Embedding(512, 1280)\n",
              "    (embeddings): Embedding(200000, 1280, padding_idx=2)\n",
              "    (layer_norm_emb): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
              "    (attentions): ModuleList(\n",
              "      (0): MultiHeadAttention(\n",
              "        (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "      )\n",
              "      (1): MultiHeadAttention(\n",
              "        (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "      )\n",
              "      (2): MultiHeadAttention(\n",
              "        (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "      )\n",
              "      (3): MultiHeadAttention(\n",
              "        (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "      )\n",
              "      (4): MultiHeadAttention(\n",
              "        (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "      )\n",
              "      (5): MultiHeadAttention(\n",
              "        (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "      )\n",
              "      (6): MultiHeadAttention(\n",
              "        (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "      )\n",
              "      (7): MultiHeadAttention(\n",
              "        (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "      )\n",
              "      (8): MultiHeadAttention(\n",
              "        (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "      )\n",
              "      (9): MultiHeadAttention(\n",
              "        (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "      )\n",
              "      (10): MultiHeadAttention(\n",
              "        (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "      )\n",
              "      (11): MultiHeadAttention(\n",
              "        (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "      )\n",
              "      (12): MultiHeadAttention(\n",
              "        (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "      )\n",
              "      (13): MultiHeadAttention(\n",
              "        (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "      )\n",
              "      (14): MultiHeadAttention(\n",
              "        (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "      )\n",
              "      (15): MultiHeadAttention(\n",
              "        (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (layer_norm1): ModuleList(\n",
              "      (0): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
              "      (1): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
              "      (2): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
              "      (3): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
              "      (4): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
              "      (5): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
              "      (6): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
              "      (7): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
              "      (8): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
              "      (9): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
              "      (10): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
              "      (11): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
              "      (12): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
              "      (13): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
              "      (14): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
              "      (15): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (ffns): ModuleList(\n",
              "      (0): TransformerFFN(\n",
              "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "      )\n",
              "      (1): TransformerFFN(\n",
              "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "      )\n",
              "      (2): TransformerFFN(\n",
              "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "      )\n",
              "      (3): TransformerFFN(\n",
              "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "      )\n",
              "      (4): TransformerFFN(\n",
              "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "      )\n",
              "      (5): TransformerFFN(\n",
              "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "      )\n",
              "      (6): TransformerFFN(\n",
              "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "      )\n",
              "      (7): TransformerFFN(\n",
              "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "      )\n",
              "      (8): TransformerFFN(\n",
              "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "      )\n",
              "      (9): TransformerFFN(\n",
              "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "      )\n",
              "      (10): TransformerFFN(\n",
              "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "      )\n",
              "      (11): TransformerFFN(\n",
              "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "      )\n",
              "      (12): TransformerFFN(\n",
              "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "      )\n",
              "      (13): TransformerFFN(\n",
              "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "      )\n",
              "      (14): TransformerFFN(\n",
              "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "      )\n",
              "      (15): TransformerFFN(\n",
              "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (layer_norm2): ModuleList(\n",
              "      (0): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
              "      (1): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
              "      (2): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
              "      (3): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
              "      (4): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
              "      (5): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
              "      (6): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
              "      (7): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
              "      (8): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
              "      (9): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
              "      (10): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
              "      (11): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
              "      (12): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
              "      (13): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
              "      (14): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
              "      (15): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (sequence_summary): SequenceSummary(\n",
              "    (summary): Linear(in_features=1280, out_features=2, bias=True)\n",
              "    (activation): Identity()\n",
              "    (first_dropout): Dropout(p=0.1, inplace=False)\n",
              "    (last_dropout): Identity()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "-e0hVzfej61w",
        "outputId": "2540f6fb-e8eb-4200-bb52-db3f6e79a3ec"
      },
      "source": [
        "texts = train['clean_text'].values\n",
        "labels = train['info'].values\n",
        "\n",
        "def plot_sentence_embeddings_length(text_list, tokenizer):\n",
        "    tokenized_texts = list(map(lambda t: tokenizer.tokenize(t), text_list))\n",
        "    tokenized_texts_len = list(map(lambda t: len(t), tokenized_texts))\n",
        "    fig, ax = plt.subplots(figsize=(8, 5));\n",
        "    ax.hist(tokenized_texts_len, bins=40);\n",
        "    ax.set_xlabel(\"Length of Comment Embeddings\");\n",
        "    ax.set_ylabel(\"Number of Comments\");\n",
        "    plt.xlim(0, 140)\n",
        "    return\n",
        "plot_sentence_embeddings_length(texts, tokenizer)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fwr2nxWvj645",
        "outputId": "bc27a6b6-1eb3-470e-88b2-d779bd9e7242"
      },
      "source": [
        "%time\n",
        "indices=tokenizer.batch_encode_plus(texts,max_length=120,add_special_tokens=True, return_attention_mask=True,pad_to_max_length=True,truncation=True)\n",
        "\n",
        "input_ids=indices[\"input_ids\"]\n",
        "attention_masks=indices[\"attention_mask\"]\n",
        "\n",
        "# Use 99% for training and 1% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=42, test_size=0.1)\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=42, test_size=0.1)\n",
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
        "validation_labels = torch.tensor(validation_labels, dtype=torch.long)\n",
        "train_masks = torch.tensor(train_masks, dtype=torch.long)\n",
        "validation_masks = torch.tensor(validation_masks, dtype=torch.long)\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
            "Wall time: 6.91 µs\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2179: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2aEjI2RlWRO",
        "outputId": "f139b226-b753-48d2-b021-171ebd2dab03"
      },
      "source": [
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "optimizer = AdamW(model.parameters(),lr = 6e-6, eps = 1e-8)\n",
        "epochs = 3\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    print('\\n======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "    t0 = time.time()\n",
        "\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if step % 50 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "        b_input_ids = batch[0].to(device)  #   [0]: input ids \n",
        "        b_input_mask = batch[1].to(device) #   [1]: attention masks\n",
        "        b_labels = batch[2].to(device)     #   [2]: labels \n",
        "        model.zero_grad() # Always clear any previously calculated gradients before performing a backward pass using pytorch    \n",
        "        # Perform a forward pass (evaluate the model on this training batch). This will return the loss\n",
        "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)        \n",
        "        loss = outputs[0] # The call to `model` always returns a tuple, so we need to pull the loss value out of the tuple.\n",
        "        total_loss += loss.item()  # Accumulate the training loss over all of the batches for calculate the average loss at the end\n",
        "        loss.backward() # Perform a backward pass to calculate the gradients.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Clip the norm of the gradients to 1.0. to help prevent the \"exploding gradients\" problem.\n",
        "        optimizer.step() # Update parameters and take a step using the computed gradient.\n",
        "        scheduler.step() # Update the learning rate.\n",
        "    avg_train_loss = total_loss / len(train_dataloader) # Calculate the average loss over the training data.            \n",
        "    loss_values.append(avg_train_loss) # Store the loss value for plotting the learning curve.\n",
        "\n",
        "    print(\"\\n  Average training loss: {0:.2f}\".format(avg_train_loss), \"  Training epoch took: {:}\\n\".format(format_time(time.time() - t0)))\n",
        "print(\"Training complete!\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "  Batch    50  of  6,674.    Elapsed: 0:01:00.\n",
            "  Batch   100  of  6,674.    Elapsed: 0:02:03.\n",
            "  Batch   150  of  6,674.    Elapsed: 0:03:07.\n",
            "  Batch   200  of  6,674.    Elapsed: 0:04:11.\n",
            "  Batch   250  of  6,674.    Elapsed: 0:05:16.\n",
            "  Batch   300  of  6,674.    Elapsed: 0:06:22.\n",
            "  Batch   350  of  6,674.    Elapsed: 0:07:28.\n",
            "  Batch   400  of  6,674.    Elapsed: 0:08:34.\n",
            "  Batch   450  of  6,674.    Elapsed: 0:09:39.\n",
            "  Batch   500  of  6,674.    Elapsed: 0:10:45.\n",
            "  Batch   550  of  6,674.    Elapsed: 0:11:51.\n",
            "  Batch   600  of  6,674.    Elapsed: 0:12:57.\n",
            "  Batch   650  of  6,674.    Elapsed: 0:14:03.\n",
            "  Batch   700  of  6,674.    Elapsed: 0:15:09.\n",
            "  Batch   750  of  6,674.    Elapsed: 0:16:15.\n",
            "  Batch   800  of  6,674.    Elapsed: 0:17:20.\n",
            "  Batch   850  of  6,674.    Elapsed: 0:18:26.\n",
            "  Batch   900  of  6,674.    Elapsed: 0:19:32.\n",
            "  Batch   950  of  6,674.    Elapsed: 0:20:38.\n",
            "  Batch 1,000  of  6,674.    Elapsed: 0:21:44.\n",
            "  Batch 1,050  of  6,674.    Elapsed: 0:22:50.\n",
            "  Batch 1,100  of  6,674.    Elapsed: 0:23:56.\n",
            "  Batch 1,150  of  6,674.    Elapsed: 0:25:02.\n",
            "  Batch 1,200  of  6,674.    Elapsed: 0:26:08.\n",
            "  Batch 1,250  of  6,674.    Elapsed: 0:27:14.\n",
            "  Batch 1,300  of  6,674.    Elapsed: 0:28:19.\n",
            "  Batch 1,350  of  6,674.    Elapsed: 0:29:25.\n",
            "  Batch 1,400  of  6,674.    Elapsed: 0:30:31.\n",
            "  Batch 1,450  of  6,674.    Elapsed: 0:31:37.\n",
            "  Batch 1,500  of  6,674.    Elapsed: 0:32:42.\n",
            "  Batch 1,550  of  6,674.    Elapsed: 0:33:48.\n",
            "  Batch 1,600  of  6,674.    Elapsed: 0:34:54.\n",
            "  Batch 1,650  of  6,674.    Elapsed: 0:36:00.\n",
            "  Batch 1,700  of  6,674.    Elapsed: 0:37:05.\n",
            "  Batch 1,750  of  6,674.    Elapsed: 0:38:11.\n",
            "  Batch 1,800  of  6,674.    Elapsed: 0:39:16.\n",
            "  Batch 1,850  of  6,674.    Elapsed: 0:40:22.\n",
            "  Batch 1,900  of  6,674.    Elapsed: 0:41:28.\n",
            "  Batch 1,950  of  6,674.    Elapsed: 0:42:34.\n",
            "  Batch 2,000  of  6,674.    Elapsed: 0:43:40.\n",
            "  Batch 2,050  of  6,674.    Elapsed: 0:44:45.\n",
            "  Batch 2,100  of  6,674.    Elapsed: 0:45:51.\n",
            "  Batch 2,150  of  6,674.    Elapsed: 0:46:57.\n",
            "  Batch 2,200  of  6,674.    Elapsed: 0:48:03.\n",
            "  Batch 2,250  of  6,674.    Elapsed: 0:49:09.\n",
            "  Batch 2,300  of  6,674.    Elapsed: 0:50:14.\n",
            "  Batch 2,350  of  6,674.    Elapsed: 0:51:20.\n",
            "  Batch 2,400  of  6,674.    Elapsed: 0:52:26.\n",
            "  Batch 2,450  of  6,674.    Elapsed: 0:53:31.\n",
            "  Batch 2,500  of  6,674.    Elapsed: 0:54:37.\n",
            "  Batch 2,550  of  6,674.    Elapsed: 0:55:43.\n",
            "  Batch 2,600  of  6,674.    Elapsed: 0:56:48.\n",
            "  Batch 2,650  of  6,674.    Elapsed: 0:57:54.\n",
            "  Batch 2,700  of  6,674.    Elapsed: 0:58:59.\n",
            "  Batch 2,750  of  6,674.    Elapsed: 1:00:05.\n",
            "  Batch 2,800  of  6,674.    Elapsed: 1:01:10.\n",
            "  Batch 2,850  of  6,674.    Elapsed: 1:02:16.\n",
            "  Batch 2,900  of  6,674.    Elapsed: 1:03:22.\n",
            "  Batch 2,950  of  6,674.    Elapsed: 1:04:28.\n",
            "  Batch 3,000  of  6,674.    Elapsed: 1:05:34.\n",
            "  Batch 3,050  of  6,674.    Elapsed: 1:06:39.\n",
            "  Batch 3,100  of  6,674.    Elapsed: 1:07:45.\n",
            "  Batch 3,150  of  6,674.    Elapsed: 1:08:51.\n",
            "  Batch 3,200  of  6,674.    Elapsed: 1:09:56.\n",
            "  Batch 3,250  of  6,674.    Elapsed: 1:11:02.\n",
            "  Batch 3,300  of  6,674.    Elapsed: 1:12:07.\n",
            "  Batch 3,350  of  6,674.    Elapsed: 1:13:13.\n",
            "  Batch 3,400  of  6,674.    Elapsed: 1:14:19.\n",
            "  Batch 3,450  of  6,674.    Elapsed: 1:15:24.\n",
            "  Batch 3,500  of  6,674.    Elapsed: 1:16:30.\n",
            "  Batch 3,550  of  6,674.    Elapsed: 1:17:35.\n",
            "  Batch 3,600  of  6,674.    Elapsed: 1:18:41.\n",
            "  Batch 3,650  of  6,674.    Elapsed: 1:19:46.\n",
            "  Batch 3,700  of  6,674.    Elapsed: 1:20:52.\n",
            "  Batch 3,750  of  6,674.    Elapsed: 1:21:58.\n",
            "  Batch 3,800  of  6,674.    Elapsed: 1:23:03.\n",
            "  Batch 3,850  of  6,674.    Elapsed: 1:24:09.\n",
            "  Batch 3,900  of  6,674.    Elapsed: 1:25:14.\n",
            "  Batch 3,950  of  6,674.    Elapsed: 1:26:20.\n",
            "  Batch 4,000  of  6,674.    Elapsed: 1:27:25.\n",
            "  Batch 4,050  of  6,674.    Elapsed: 1:28:31.\n",
            "  Batch 4,100  of  6,674.    Elapsed: 1:29:37.\n",
            "  Batch 4,150  of  6,674.    Elapsed: 1:30:43.\n",
            "  Batch 4,200  of  6,674.    Elapsed: 1:31:49.\n",
            "  Batch 4,250  of  6,674.    Elapsed: 1:32:54.\n",
            "  Batch 4,300  of  6,674.    Elapsed: 1:34:00.\n",
            "  Batch 4,350  of  6,674.    Elapsed: 1:35:06.\n",
            "  Batch 4,400  of  6,674.    Elapsed: 1:36:11.\n",
            "  Batch 4,450  of  6,674.    Elapsed: 1:37:17.\n",
            "  Batch 4,500  of  6,674.    Elapsed: 1:38:23.\n",
            "  Batch 4,550  of  6,674.    Elapsed: 1:39:28.\n",
            "  Batch 4,600  of  6,674.    Elapsed: 1:40:34.\n",
            "  Batch 4,650  of  6,674.    Elapsed: 1:41:40.\n",
            "  Batch 4,700  of  6,674.    Elapsed: 1:42:45.\n",
            "  Batch 4,750  of  6,674.    Elapsed: 1:43:50.\n",
            "  Batch 4,800  of  6,674.    Elapsed: 1:44:56.\n",
            "  Batch 4,850  of  6,674.    Elapsed: 1:46:01.\n",
            "  Batch 4,900  of  6,674.    Elapsed: 1:47:07.\n",
            "  Batch 4,950  of  6,674.    Elapsed: 1:48:13.\n",
            "  Batch 5,000  of  6,674.    Elapsed: 1:49:19.\n",
            "  Batch 5,050  of  6,674.    Elapsed: 1:50:25.\n",
            "  Batch 5,100  of  6,674.    Elapsed: 1:51:30.\n",
            "  Batch 5,150  of  6,674.    Elapsed: 1:52:36.\n",
            "  Batch 5,200  of  6,674.    Elapsed: 1:53:42.\n",
            "  Batch 5,250  of  6,674.    Elapsed: 1:54:47.\n",
            "  Batch 5,300  of  6,674.    Elapsed: 1:55:53.\n",
            "  Batch 5,350  of  6,674.    Elapsed: 1:56:59.\n",
            "  Batch 5,400  of  6,674.    Elapsed: 1:58:05.\n",
            "  Batch 5,450  of  6,674.    Elapsed: 1:59:11.\n",
            "  Batch 5,500  of  6,674.    Elapsed: 2:00:17.\n",
            "  Batch 5,550  of  6,674.    Elapsed: 2:01:23.\n",
            "  Batch 5,600  of  6,674.    Elapsed: 2:02:29.\n",
            "  Batch 5,650  of  6,674.    Elapsed: 2:03:34.\n",
            "  Batch 5,700  of  6,674.    Elapsed: 2:04:40.\n",
            "  Batch 5,750  of  6,674.    Elapsed: 2:05:46.\n",
            "  Batch 5,800  of  6,674.    Elapsed: 2:06:52.\n",
            "  Batch 5,850  of  6,674.    Elapsed: 2:07:57.\n",
            "  Batch 5,900  of  6,674.    Elapsed: 2:09:03.\n",
            "  Batch 5,950  of  6,674.    Elapsed: 2:10:09.\n",
            "  Batch 6,000  of  6,674.    Elapsed: 2:11:15.\n",
            "  Batch 6,050  of  6,674.    Elapsed: 2:12:20.\n",
            "  Batch 6,100  of  6,674.    Elapsed: 2:13:26.\n",
            "  Batch 6,150  of  6,674.    Elapsed: 2:14:32.\n",
            "  Batch 6,200  of  6,674.    Elapsed: 2:15:37.\n",
            "  Batch 6,250  of  6,674.    Elapsed: 2:16:43.\n",
            "  Batch 6,300  of  6,674.    Elapsed: 2:17:49.\n",
            "  Batch 6,350  of  6,674.    Elapsed: 2:18:54.\n",
            "  Batch 6,400  of  6,674.    Elapsed: 2:20:00.\n",
            "  Batch 6,450  of  6,674.    Elapsed: 2:21:06.\n",
            "  Batch 6,500  of  6,674.    Elapsed: 2:22:11.\n",
            "  Batch 6,550  of  6,674.    Elapsed: 2:23:17.\n",
            "  Batch 6,600  of  6,674.    Elapsed: 2:24:22.\n",
            "  Batch 6,650  of  6,674.    Elapsed: 2:25:28.\n",
            "\n",
            "  Average training loss: 0.11   Training epoch took: 2:25:59\n",
            "\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "  Batch    50  of  6,674.    Elapsed: 0:01:05.\n",
            "  Batch   100  of  6,674.    Elapsed: 0:02:11.\n",
            "  Batch   150  of  6,674.    Elapsed: 0:03:16.\n",
            "  Batch   200  of  6,674.    Elapsed: 0:04:22.\n",
            "  Batch   250  of  6,674.    Elapsed: 0:05:27.\n",
            "  Batch   300  of  6,674.    Elapsed: 0:06:33.\n",
            "  Batch   350  of  6,674.    Elapsed: 0:07:38.\n",
            "  Batch   400  of  6,674.    Elapsed: 0:08:44.\n",
            "  Batch   450  of  6,674.    Elapsed: 0:09:49.\n",
            "  Batch   500  of  6,674.    Elapsed: 0:10:55.\n",
            "  Batch   550  of  6,674.    Elapsed: 0:12:00.\n",
            "  Batch   600  of  6,674.    Elapsed: 0:13:06.\n",
            "  Batch   650  of  6,674.    Elapsed: 0:14:11.\n",
            "  Batch   700  of  6,674.    Elapsed: 0:15:16.\n",
            "  Batch   750  of  6,674.    Elapsed: 0:16:22.\n",
            "  Batch   800  of  6,674.    Elapsed: 0:17:27.\n",
            "  Batch   850  of  6,674.    Elapsed: 0:18:33.\n",
            "  Batch   900  of  6,674.    Elapsed: 0:19:38.\n",
            "  Batch   950  of  6,674.    Elapsed: 0:20:44.\n",
            "  Batch 1,000  of  6,674.    Elapsed: 0:21:49.\n",
            "  Batch 1,050  of  6,674.    Elapsed: 0:22:55.\n",
            "  Batch 1,100  of  6,674.    Elapsed: 0:24:01.\n",
            "  Batch 1,150  of  6,674.    Elapsed: 0:25:06.\n",
            "  Batch 1,200  of  6,674.    Elapsed: 0:26:11.\n",
            "  Batch 1,250  of  6,674.    Elapsed: 0:27:17.\n",
            "  Batch 1,300  of  6,674.    Elapsed: 0:28:23.\n",
            "  Batch 1,350  of  6,674.    Elapsed: 0:29:29.\n",
            "  Batch 1,400  of  6,674.    Elapsed: 0:30:35.\n",
            "  Batch 1,450  of  6,674.    Elapsed: 0:31:41.\n",
            "  Batch 1,500  of  6,674.    Elapsed: 0:32:47.\n",
            "  Batch 1,550  of  6,674.    Elapsed: 0:33:53.\n",
            "  Batch 1,600  of  6,674.    Elapsed: 0:34:59.\n",
            "  Batch 1,650  of  6,674.    Elapsed: 0:36:05.\n",
            "  Batch 1,700  of  6,674.    Elapsed: 0:37:11.\n",
            "  Batch 1,750  of  6,674.    Elapsed: 0:38:17.\n",
            "  Batch 1,800  of  6,674.    Elapsed: 0:39:23.\n",
            "  Batch 1,850  of  6,674.    Elapsed: 0:40:29.\n",
            "  Batch 1,900  of  6,674.    Elapsed: 0:41:35.\n",
            "  Batch 1,950  of  6,674.    Elapsed: 0:42:41.\n",
            "  Batch 2,000  of  6,674.    Elapsed: 0:43:48.\n",
            "  Batch 2,050  of  6,674.    Elapsed: 0:44:54.\n",
            "  Batch 2,100  of  6,674.    Elapsed: 0:46:00.\n",
            "  Batch 2,150  of  6,674.    Elapsed: 0:47:06.\n",
            "  Batch 2,200  of  6,674.    Elapsed: 0:48:13.\n",
            "  Batch 2,250  of  6,674.    Elapsed: 0:49:19.\n",
            "  Batch 2,300  of  6,674.    Elapsed: 0:50:25.\n",
            "  Batch 2,350  of  6,674.    Elapsed: 0:51:31.\n",
            "  Batch 2,400  of  6,674.    Elapsed: 0:52:37.\n",
            "  Batch 2,450  of  6,674.    Elapsed: 0:53:43.\n",
            "  Batch 2,500  of  6,674.    Elapsed: 0:54:50.\n",
            "  Batch 2,550  of  6,674.    Elapsed: 0:55:56.\n",
            "  Batch 2,600  of  6,674.    Elapsed: 0:57:02.\n",
            "  Batch 2,650  of  6,674.    Elapsed: 0:58:08.\n",
            "  Batch 2,700  of  6,674.    Elapsed: 0:59:14.\n",
            "  Batch 2,750  of  6,674.    Elapsed: 1:00:20.\n",
            "  Batch 2,800  of  6,674.    Elapsed: 1:01:27.\n",
            "  Batch 2,850  of  6,674.    Elapsed: 1:02:33.\n",
            "  Batch 2,900  of  6,674.    Elapsed: 1:03:39.\n",
            "  Batch 2,950  of  6,674.    Elapsed: 1:04:45.\n",
            "  Batch 3,000  of  6,674.    Elapsed: 1:05:51.\n",
            "  Batch 3,050  of  6,674.    Elapsed: 1:06:57.\n",
            "  Batch 3,100  of  6,674.    Elapsed: 1:08:03.\n",
            "  Batch 3,150  of  6,674.    Elapsed: 1:09:09.\n",
            "  Batch 3,200  of  6,674.    Elapsed: 1:10:15.\n",
            "  Batch 3,250  of  6,674.    Elapsed: 1:11:21.\n",
            "  Batch 3,300  of  6,674.    Elapsed: 1:12:27.\n",
            "  Batch 3,350  of  6,674.    Elapsed: 1:13:34.\n",
            "  Batch 3,400  of  6,674.    Elapsed: 1:14:40.\n",
            "  Batch 3,450  of  6,674.    Elapsed: 1:15:46.\n",
            "  Batch 3,500  of  6,674.    Elapsed: 1:16:52.\n",
            "  Batch 3,550  of  6,674.    Elapsed: 1:17:58.\n",
            "  Batch 3,600  of  6,674.    Elapsed: 1:19:04.\n",
            "  Batch 3,650  of  6,674.    Elapsed: 1:20:11.\n",
            "  Batch 3,700  of  6,674.    Elapsed: 1:21:17.\n",
            "  Batch 3,750  of  6,674.    Elapsed: 1:22:23.\n",
            "  Batch 3,800  of  6,674.    Elapsed: 1:23:29.\n",
            "  Batch 3,850  of  6,674.    Elapsed: 1:24:35.\n",
            "  Batch 3,900  of  6,674.    Elapsed: 1:25:41.\n",
            "  Batch 3,950  of  6,674.    Elapsed: 1:26:47.\n",
            "  Batch 4,000  of  6,674.    Elapsed: 1:27:53.\n",
            "  Batch 4,050  of  6,674.    Elapsed: 1:29:00.\n",
            "  Batch 4,100  of  6,674.    Elapsed: 1:30:06.\n",
            "  Batch 4,150  of  6,674.    Elapsed: 1:31:12.\n",
            "  Batch 4,200  of  6,674.    Elapsed: 1:32:18.\n",
            "  Batch 4,250  of  6,674.    Elapsed: 1:33:24.\n",
            "  Batch 4,300  of  6,674.    Elapsed: 1:34:30.\n",
            "  Batch 4,350  of  6,674.    Elapsed: 1:35:36.\n",
            "  Batch 4,400  of  6,674.    Elapsed: 1:36:43.\n",
            "  Batch 4,450  of  6,674.    Elapsed: 1:37:49.\n",
            "  Batch 4,500  of  6,674.    Elapsed: 1:38:55.\n",
            "  Batch 4,550  of  6,674.    Elapsed: 1:40:01.\n",
            "  Batch 4,600  of  6,674.    Elapsed: 1:41:07.\n",
            "  Batch 4,650  of  6,674.    Elapsed: 1:42:13.\n",
            "  Batch 4,700  of  6,674.    Elapsed: 1:43:19.\n",
            "  Batch 4,750  of  6,674.    Elapsed: 1:44:25.\n",
            "  Batch 4,800  of  6,674.    Elapsed: 1:45:31.\n",
            "  Batch 4,850  of  6,674.    Elapsed: 1:46:37.\n",
            "  Batch 4,900  of  6,674.    Elapsed: 1:47:43.\n",
            "  Batch 4,950  of  6,674.    Elapsed: 1:48:49.\n",
            "  Batch 5,000  of  6,674.    Elapsed: 1:49:55.\n",
            "  Batch 5,050  of  6,674.    Elapsed: 1:51:01.\n",
            "  Batch 5,100  of  6,674.    Elapsed: 1:52:07.\n",
            "  Batch 5,150  of  6,674.    Elapsed: 1:53:13.\n",
            "  Batch 5,200  of  6,674.    Elapsed: 1:54:19.\n",
            "  Batch 5,250  of  6,674.    Elapsed: 1:55:25.\n",
            "  Batch 5,300  of  6,674.    Elapsed: 1:56:31.\n",
            "  Batch 5,350  of  6,674.    Elapsed: 1:57:37.\n",
            "  Batch 5,400  of  6,674.    Elapsed: 1:58:43.\n",
            "  Batch 5,450  of  6,674.    Elapsed: 1:59:49.\n",
            "  Batch 5,500  of  6,674.    Elapsed: 2:00:56.\n",
            "  Batch 5,550  of  6,674.    Elapsed: 2:02:01.\n",
            "  Batch 5,600  of  6,674.    Elapsed: 2:03:07.\n",
            "  Batch 5,650  of  6,674.    Elapsed: 2:04:13.\n",
            "  Batch 5,700  of  6,674.    Elapsed: 2:05:19.\n",
            "  Batch 5,750  of  6,674.    Elapsed: 2:06:26.\n",
            "  Batch 5,800  of  6,674.    Elapsed: 2:07:32.\n",
            "  Batch 5,850  of  6,674.    Elapsed: 2:08:38.\n",
            "  Batch 5,900  of  6,674.    Elapsed: 2:09:44.\n",
            "  Batch 5,950  of  6,674.    Elapsed: 2:10:50.\n",
            "  Batch 6,000  of  6,674.    Elapsed: 2:11:56.\n",
            "  Batch 6,050  of  6,674.    Elapsed: 2:13:02.\n",
            "  Batch 6,100  of  6,674.    Elapsed: 2:14:08.\n",
            "  Batch 6,150  of  6,674.    Elapsed: 2:15:13.\n",
            "  Batch 6,200  of  6,674.    Elapsed: 2:16:20.\n",
            "  Batch 6,250  of  6,674.    Elapsed: 2:17:26.\n",
            "  Batch 6,300  of  6,674.    Elapsed: 2:18:32.\n",
            "  Batch 6,350  of  6,674.    Elapsed: 2:19:38.\n",
            "  Batch 6,400  of  6,674.    Elapsed: 2:20:44.\n",
            "  Batch 6,450  of  6,674.    Elapsed: 2:21:50.\n",
            "  Batch 6,500  of  6,674.    Elapsed: 2:22:56.\n",
            "  Batch 6,550  of  6,674.    Elapsed: 2:24:02.\n",
            "  Batch 6,600  of  6,674.    Elapsed: 2:25:08.\n",
            "  Batch 6,650  of  6,674.    Elapsed: 2:26:14.\n",
            "\n",
            "  Average training loss: 0.05   Training epoch took: 2:26:45\n",
            "\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "  Batch    50  of  6,674.    Elapsed: 0:01:06.\n",
            "  Batch   100  of  6,674.    Elapsed: 0:02:12.\n",
            "  Batch   150  of  6,674.    Elapsed: 0:03:18.\n",
            "  Batch   200  of  6,674.    Elapsed: 0:04:24.\n",
            "  Batch   250  of  6,674.    Elapsed: 0:05:30.\n",
            "  Batch   300  of  6,674.    Elapsed: 0:06:36.\n",
            "  Batch   350  of  6,674.    Elapsed: 0:07:42.\n",
            "  Batch   400  of  6,674.    Elapsed: 0:08:48.\n",
            "  Batch   450  of  6,674.    Elapsed: 0:09:54.\n",
            "  Batch   500  of  6,674.    Elapsed: 0:11:00.\n",
            "  Batch   550  of  6,674.    Elapsed: 0:12:06.\n",
            "  Batch   600  of  6,674.    Elapsed: 0:13:12.\n",
            "  Batch   650  of  6,674.    Elapsed: 0:14:19.\n",
            "  Batch   700  of  6,674.    Elapsed: 0:15:25.\n",
            "  Batch   750  of  6,674.    Elapsed: 0:16:31.\n",
            "  Batch   800  of  6,674.    Elapsed: 0:17:37.\n",
            "  Batch   850  of  6,674.    Elapsed: 0:18:43.\n",
            "  Batch   900  of  6,674.    Elapsed: 0:19:49.\n",
            "  Batch   950  of  6,674.    Elapsed: 0:20:55.\n",
            "  Batch 1,000  of  6,674.    Elapsed: 0:22:01.\n",
            "  Batch 1,050  of  6,674.    Elapsed: 0:23:07.\n",
            "  Batch 1,100  of  6,674.    Elapsed: 0:24:13.\n",
            "  Batch 1,150  of  6,674.    Elapsed: 0:25:19.\n",
            "  Batch 1,200  of  6,674.    Elapsed: 0:26:25.\n",
            "  Batch 1,250  of  6,674.    Elapsed: 0:27:31.\n",
            "  Batch 1,300  of  6,674.    Elapsed: 0:28:37.\n",
            "  Batch 1,350  of  6,674.    Elapsed: 0:29:43.\n",
            "  Batch 1,400  of  6,674.    Elapsed: 0:30:49.\n",
            "  Batch 1,450  of  6,674.    Elapsed: 0:31:55.\n",
            "  Batch 1,500  of  6,674.    Elapsed: 0:33:01.\n",
            "  Batch 1,550  of  6,674.    Elapsed: 0:34:07.\n",
            "  Batch 1,600  of  6,674.    Elapsed: 0:35:13.\n",
            "  Batch 1,650  of  6,674.    Elapsed: 0:36:19.\n",
            "  Batch 1,700  of  6,674.    Elapsed: 0:37:25.\n",
            "  Batch 1,750  of  6,674.    Elapsed: 0:38:31.\n",
            "  Batch 1,800  of  6,674.    Elapsed: 0:39:37.\n",
            "  Batch 1,850  of  6,674.    Elapsed: 0:40:42.\n",
            "  Batch 1,900  of  6,674.    Elapsed: 0:41:48.\n",
            "  Batch 1,950  of  6,674.    Elapsed: 0:42:54.\n",
            "  Batch 2,000  of  6,674.    Elapsed: 0:44:00.\n",
            "  Batch 2,050  of  6,674.    Elapsed: 0:45:06.\n",
            "  Batch 2,100  of  6,674.    Elapsed: 0:46:12.\n",
            "  Batch 2,150  of  6,674.    Elapsed: 0:47:18.\n",
            "  Batch 2,200  of  6,674.    Elapsed: 0:48:24.\n",
            "  Batch 2,250  of  6,674.    Elapsed: 0:49:30.\n",
            "  Batch 2,300  of  6,674.    Elapsed: 0:50:36.\n",
            "  Batch 2,350  of  6,674.    Elapsed: 0:51:42.\n",
            "  Batch 2,400  of  6,674.    Elapsed: 0:52:48.\n",
            "  Batch 2,450  of  6,674.    Elapsed: 0:53:54.\n",
            "  Batch 2,500  of  6,674.    Elapsed: 0:55:00.\n",
            "  Batch 2,550  of  6,674.    Elapsed: 0:56:06.\n",
            "  Batch 2,600  of  6,674.    Elapsed: 0:57:12.\n",
            "  Batch 2,650  of  6,674.    Elapsed: 0:58:18.\n",
            "  Batch 2,700  of  6,674.    Elapsed: 0:59:24.\n",
            "  Batch 2,750  of  6,674.    Elapsed: 1:00:30.\n",
            "  Batch 2,800  of  6,674.    Elapsed: 1:01:36.\n",
            "  Batch 2,850  of  6,674.    Elapsed: 1:02:42.\n",
            "  Batch 2,900  of  6,674.    Elapsed: 1:03:48.\n",
            "  Batch 2,950  of  6,674.    Elapsed: 1:04:54.\n",
            "  Batch 3,000  of  6,674.    Elapsed: 1:05:59.\n",
            "  Batch 3,050  of  6,674.    Elapsed: 1:07:05.\n",
            "  Batch 3,100  of  6,674.    Elapsed: 1:08:11.\n",
            "  Batch 3,150  of  6,674.    Elapsed: 1:09:17.\n",
            "  Batch 3,200  of  6,674.    Elapsed: 1:10:23.\n",
            "  Batch 3,250  of  6,674.    Elapsed: 1:11:28.\n",
            "  Batch 3,300  of  6,674.    Elapsed: 1:12:34.\n",
            "  Batch 3,350  of  6,674.    Elapsed: 1:13:40.\n",
            "  Batch 3,400  of  6,674.    Elapsed: 1:14:46.\n",
            "  Batch 3,450  of  6,674.    Elapsed: 1:15:52.\n",
            "  Batch 3,500  of  6,674.    Elapsed: 1:16:57.\n",
            "  Batch 3,550  of  6,674.    Elapsed: 1:18:03.\n",
            "  Batch 3,600  of  6,674.    Elapsed: 1:19:09.\n",
            "  Batch 3,650  of  6,674.    Elapsed: 1:20:15.\n",
            "  Batch 3,700  of  6,674.    Elapsed: 1:21:21.\n",
            "  Batch 3,750  of  6,674.    Elapsed: 1:22:27.\n",
            "  Batch 3,800  of  6,674.    Elapsed: 1:23:32.\n",
            "  Batch 3,850  of  6,674.    Elapsed: 1:24:38.\n",
            "  Batch 3,900  of  6,674.    Elapsed: 1:25:44.\n",
            "  Batch 3,950  of  6,674.    Elapsed: 1:26:50.\n",
            "  Batch 4,000  of  6,674.    Elapsed: 1:27:56.\n",
            "  Batch 4,050  of  6,674.    Elapsed: 1:29:02.\n",
            "  Batch 4,100  of  6,674.    Elapsed: 1:30:07.\n",
            "  Batch 4,150  of  6,674.    Elapsed: 1:31:13.\n",
            "  Batch 4,200  of  6,674.    Elapsed: 1:32:19.\n",
            "  Batch 4,250  of  6,674.    Elapsed: 1:33:25.\n",
            "  Batch 4,300  of  6,674.    Elapsed: 1:34:31.\n",
            "  Batch 4,350  of  6,674.    Elapsed: 1:35:37.\n",
            "  Batch 4,400  of  6,674.    Elapsed: 1:36:42.\n",
            "  Batch 4,450  of  6,674.    Elapsed: 1:37:48.\n",
            "  Batch 4,500  of  6,674.    Elapsed: 1:38:54.\n",
            "  Batch 4,550  of  6,674.    Elapsed: 1:40:00.\n",
            "  Batch 4,600  of  6,674.    Elapsed: 1:41:06.\n",
            "  Batch 4,650  of  6,674.    Elapsed: 1:42:11.\n",
            "  Batch 4,700  of  6,674.    Elapsed: 1:43:17.\n",
            "  Batch 4,750  of  6,674.    Elapsed: 1:44:23.\n",
            "  Batch 4,800  of  6,674.    Elapsed: 1:45:29.\n",
            "  Batch 4,850  of  6,674.    Elapsed: 1:46:35.\n",
            "  Batch 4,900  of  6,674.    Elapsed: 1:47:41.\n",
            "  Batch 4,950  of  6,674.    Elapsed: 1:48:47.\n",
            "  Batch 5,000  of  6,674.    Elapsed: 1:49:52.\n",
            "  Batch 5,050  of  6,674.    Elapsed: 1:50:58.\n",
            "  Batch 5,100  of  6,674.    Elapsed: 1:52:04.\n",
            "  Batch 5,150  of  6,674.    Elapsed: 1:53:10.\n",
            "  Batch 5,200  of  6,674.    Elapsed: 1:54:16.\n",
            "  Batch 5,250  of  6,674.    Elapsed: 1:55:22.\n",
            "  Batch 5,300  of  6,674.    Elapsed: 1:56:28.\n",
            "  Batch 5,350  of  6,674.    Elapsed: 1:57:34.\n",
            "  Batch 5,400  of  6,674.    Elapsed: 1:58:39.\n",
            "  Batch 5,450  of  6,674.    Elapsed: 1:59:45.\n",
            "  Batch 5,500  of  6,674.    Elapsed: 2:00:51.\n",
            "  Batch 5,550  of  6,674.    Elapsed: 2:01:57.\n",
            "  Batch 5,600  of  6,674.    Elapsed: 2:03:03.\n",
            "  Batch 5,650  of  6,674.    Elapsed: 2:04:08.\n",
            "  Batch 5,700  of  6,674.    Elapsed: 2:05:14.\n",
            "  Batch 5,750  of  6,674.    Elapsed: 2:06:20.\n",
            "  Batch 5,800  of  6,674.    Elapsed: 2:07:26.\n",
            "  Batch 5,850  of  6,674.    Elapsed: 2:08:32.\n",
            "  Batch 5,900  of  6,674.    Elapsed: 2:09:38.\n",
            "  Batch 5,950  of  6,674.    Elapsed: 2:10:43.\n",
            "  Batch 6,000  of  6,674.    Elapsed: 2:11:49.\n",
            "  Batch 6,050  of  6,674.    Elapsed: 2:12:55.\n",
            "  Batch 6,100  of  6,674.    Elapsed: 2:14:01.\n",
            "  Batch 6,150  of  6,674.    Elapsed: 2:15:07.\n",
            "  Batch 6,200  of  6,674.    Elapsed: 2:16:13.\n",
            "  Batch 6,250  of  6,674.    Elapsed: 2:17:19.\n",
            "  Batch 6,300  of  6,674.    Elapsed: 2:18:24.\n",
            "  Batch 6,350  of  6,674.    Elapsed: 2:19:30.\n",
            "  Batch 6,400  of  6,674.    Elapsed: 2:20:36.\n",
            "  Batch 6,450  of  6,674.    Elapsed: 2:21:42.\n",
            "  Batch 6,500  of  6,674.    Elapsed: 2:22:48.\n",
            "  Batch 6,550  of  6,674.    Elapsed: 2:23:54.\n",
            "  Batch 6,600  of  6,674.    Elapsed: 2:24:59.\n",
            "  Batch 6,650  of  6,674.    Elapsed: 2:26:06.\n",
            "\n",
            "  Average training loss: 0.02   Training epoch took: 2:26:37\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceT0l4mCFU7p"
      },
      "source": [
        "def flat_accuracy(preds, labels):\n",
        "    \n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJZfzaZLlWT-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cd6825f-40bb-40af-a1f5-5f914b64615d"
      },
      "source": [
        "print(\"\\nRunning Validation...\")\n",
        "t0 = time.time()\n",
        "model.eval() # Put the model in evaluation mode--the dropout layers behave differently during evaluation.\n",
        "preds=[]\n",
        "true=[]\n",
        "# Tracking variables \n",
        "eval_loss, eval_accuracy = 0, 0\n",
        "nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "# Evaluate data for one epoch\n",
        "for batch in validation_dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch) # Add batch to GPU\n",
        "    b_input_ids, b_input_mask, b_labels = batch # Unpack the inputs from our dataloader\n",
        "    with torch.no_grad(): # Telling the model not to compute or store gradients, saving memory and speeding up validation      \n",
        "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask) # Forward pass, calculate logit predictions, cuz not with labels\n",
        "    \n",
        "    logits = outputs[0] # values prior to applying an activation function like the softmax.\n",
        "    logits = logits.detach().cpu().numpy() # Move logits and labels to CPU\n",
        "    label_ids = b_labels.to('cpu').numpy() # Move logits and labels to CPU\n",
        "    preds.append(logits)\n",
        "    true.append(label_ids)\n",
        "    \n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids) # Calculate the accuracy for this batch of test sentences.\n",
        "    eval_accuracy += tmp_eval_accuracy # Accumulate the total accuracy\n",
        "    nb_eval_steps += 1 # Track the number of batches\n",
        "\n",
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in preds for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true for item in sublist]\n",
        "\n",
        "print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "print(\"  Validation took: {:}\\n\".format(format_time(time.time() - t0)))\n",
        "print(\"  Classification report\\n\",classification_report(flat_predictions,flat_true_labels))\n",
        "print(\"  Model saving....\")\n",
        "\n",
        "torch.save(model, '/content/drive/My Drive/dacon/xlm_1280.pt')  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.99\n",
            "  Validation took: 0:05:14\n",
            "\n",
            "  Classification report\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99      7151\n",
            "           1       0.98      0.99      0.98      4713\n",
            "\n",
            "    accuracy                           0.99     11864\n",
            "   macro avg       0.99      0.99      0.99     11864\n",
            "weighted avg       0.99      0.99      0.99     11864\n",
            "\n",
            "  Model saving....\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzoq4BG6Dv4I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bab317da-6643-4388-b2db-c49ae506defc"
      },
      "source": [
        "comments1 = test['clean_text'].values\n",
        "\n",
        "indices1=tokenizer.batch_encode_plus(comments1,max_length=120,add_special_tokens=True, return_attention_mask=True,pad_to_max_length=True,truncation=True)\n",
        "input_ids1=indices1[\"input_ids\"]\n",
        "attention_masks1=indices1[\"attention_mask\"]\n",
        "\n",
        "prediction_inputs1= torch.tensor(input_ids1)\n",
        "prediction_masks1 = torch.tensor(attention_masks1)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data1 = TensorDataset(prediction_inputs1, prediction_masks1)\n",
        "prediction_sampler1 = SequentialSampler(prediction_data1)\n",
        "prediction_dataloader1 = DataLoader(prediction_data1, sampler=prediction_sampler1, batch_size=batch_size)\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs1)))\n",
        "\n",
        "model.eval()\n",
        "predictions = []\n",
        "for batch in prediction_dataloader1:\n",
        "  batch = tuple(t.to(device) for t in batch) # Add batch to GPU\n",
        "  b_input_ids1, b_input_mask1 = batch # Unpack the inputs from our dataloader\n",
        "  with torch.no_grad(): # Telling the model not to compute or store gradients, saving memory and speeding up prediction \n",
        "      outputs1 = model(b_input_ids1, token_type_ids=None, attention_mask=b_input_mask1) # Forward pass, calculate logit predictions\n",
        "  logits1 = outputs1[0]\n",
        "  logits1 = logits1.detach().cpu().numpy() # Move logits and labels to CPU\n",
        "  predictions.append(logits1) # Store predictions and true labels\n",
        "\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "sample_sub=pd.read_csv('/content/drive/My Drive/dacon/sample_submission.csv')\n",
        "submit=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'info':flat_predictions})\n",
        "submit.to_csv('/content/drive/My Drive/dacon/xlm_1280.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2179: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 142,565 test sentences...\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}