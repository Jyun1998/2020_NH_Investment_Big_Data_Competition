{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "[채점_양식] python version.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27XED42TgYIX",
        "outputId": "be8b8d44-0638-4313-f2be-43b36badd24e"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 7.9MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 44.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 39.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=3cde6efb23b947464e69ec3c0d256d458baeabdcef3abf50fe90589894837291\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j35bH-TkAQvP"
      },
      "source": [
        "# torch.load시 cuda 오류가 나서 모델 훈련하는 부분을 구현한 후 content 폴더에 model.pt로 저장한 후 속도 계산 부분에서 이 model.pt를 불러오는 것으로 대체하였습니다.\n",
        "\n",
        "*kaggle, local 등 환경에서는 문제 x (코랩 기준 1시간 걸립니다)\n",
        "\n",
        "content/news_train.csv 넣어주셔야 합니다\n",
        "\n",
        "*이후 추론 시간 측정 시 공정한 시간 평가를 위해 라이버리 로딩은 다시 넣었습니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUdNQao771Wl",
        "outputId": "447e4e38-7402-4aac-86ba-cfaf84b1302e"
      },
      "source": [
        "import re\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import numpy as np      \n",
        "import pandas as pd       \n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "if torch.cuda.is_available():  \n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "tokenizer = ElectraTokenizer.from_pretrained('monologg/koelectra-base-v3-discriminator')\n",
        "model = ElectraForSequenceClassification.from_pretrained('monologg/koelectra-base-v3-discriminator',num_labels=2)\n",
        "model.cuda()\n",
        "train = pd.read_csv(\"/content/news_train.csv\")\n",
        "def preprocess(text):\n",
        "    # remove hyperlinks\n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n",
        "    text = re.sub(r'http?:\\/\\/.*[\\r\\n]*', '', text)\n",
        "    #remove some puncts (except . ! ?)\n",
        "    text=re.sub(r'[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', '',text)\n",
        "    text=\" \".join(text.split())\n",
        "    return text\n",
        "train['clean_text'] = train['content'].apply(preprocess)\n",
        "texts = train['clean_text'].values\n",
        "labels = train['info'].values\n",
        "indices=tokenizer.batch_encode_plus(texts,max_length=120,add_special_tokens=True, padding='max_length',pad_to_max_length=True,truncation=True)\n",
        "\n",
        "input_ids=indices[\"input_ids\"]\n",
        "attention_masks=indices[\"attention_mask\"]\n",
        "\n",
        "# Use 99% for training and 1% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=42)\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=42)\n",
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
        "validation_labels = torch.tensor(validation_labels, dtype=torch.long)\n",
        "train_masks = torch.tensor(train_masks, dtype=torch.long)\n",
        "validation_masks = torch.tensor(validation_masks, dtype=torch.long)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "optimizer = AdamW(model.parameters(),lr = 6e-6, eps = 1e-8)\n",
        "epochs = 3\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)\n",
        "loss_values = []\n",
        "\n",
        "print('Training...')\n",
        "for epoch_i in range(0, epochs):\n",
        "    print('\\n======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    t0 = time.time()\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if step % 1000 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "        b_input_ids = batch[0].to(device)  #   [0]: input ids \n",
        "        b_input_mask = batch[1].to(device) #   [1]: attention masks\n",
        "        b_labels = batch[2].to(device)     #   [2]: labels \n",
        "        model.zero_grad() # Always clear any previously calculated gradients before performing a backward pass using pytorch    \n",
        "        # Perform a forward pass (evaluate the model on this training batch). This will return the loss\n",
        "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)        \n",
        "        loss = outputs[0] # The call to `model` always returns a tuple, so we need to pull the loss value out of the tuple.\n",
        "        total_loss += loss.item()  # Accumulate the training loss over all of the batches for calculate the average loss at the end\n",
        "        loss.backward() # Perform a backward pass to calculate the gradients.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Clip the norm of the gradients to 1.0. to help prevent the \"exploding gradients\" problem.\n",
        "        optimizer.step() # Update parameters and take a step using the computed gradient.\n",
        "        scheduler.step() # Update the learning rate.\n",
        "    avg_train_loss = total_loss / len(train_dataloader) # Calculate the average loss over the training data.            \n",
        "    loss_values.append(avg_train_loss) # Store the loss value for plotting the learning curve.\n",
        "print(\"Training complete!\")\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "print(\"\\nRunning Validation...\")\n",
        "t0 = time.time()\n",
        "model.eval() # Put the model in evaluation mode--the dropout layers behave differently during evaluation.\n",
        "preds=[]\n",
        "true=[]\n",
        "# Tracking variables \n",
        "eval_loss, eval_accuracy = 0, 0\n",
        "nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "# Evaluate data for one epoch\n",
        "for batch in validation_dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch) # Add batch to GPU\n",
        "    b_input_ids, b_input_mask, b_labels = batch # Unpack the inputs from our dataloader\n",
        "    with torch.no_grad(): # Telling the model not to compute or store gradients, saving memory and speeding up validation      \n",
        "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask) # Forward pass, calculate logit predictions, cuz not with labels\n",
        "    \n",
        "    logits = outputs[0] # values prior to applying an activation function like the softmax.\n",
        "    logits = logits.detach().cpu().numpy() # Move logits and labels to CPU\n",
        "    label_ids = b_labels.to('cpu').numpy() # Move logits and labels to CPU\n",
        "    preds.append(logits)\n",
        "    true.append(label_ids)\n",
        "    \n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids) # Calculate the accuracy for this batch of test sentences.\n",
        "    eval_accuracy += tmp_eval_accuracy # Accumulate the total accuracy\n",
        "    nb_eval_steps += 1 # Track the number of batches\n",
        "\n",
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in preds for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true for item in sublist]\n",
        "#print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "#print(\"  Validation took: {:}\\n\".format(format_time(time.time() - t0)))\n",
        "#print(\"  Classification report\\n\",classification_report(flat_predictions,flat_true_labels))\n",
        "print(\"  Model saving....\")\n",
        "torch.save(model,'/content/model.pt')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
            "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "Training...\n",
            "Training...\n",
            "Training complete!\n",
            "\n",
            "Running Validation...\n",
            "  Model saving....\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLSmwxh5oYcR"
      },
      "source": [
        "## 데이터 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG_9f-dXoXxn"
      },
      "source": [
        "import pandas as pd       \n",
        "test = pd.read_csv('/content/news_test.csv')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HFsSSA5gIG0"
      },
      "source": [
        "## 시간 측정 시작"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0JauilKf7G0"
      },
      "source": [
        "import time\n",
        "start = time.time()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vixMPEbszAiS"
      },
      "source": [
        "## Library  불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7VthfdXzERD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fda5f0c-248e-4a5d-dd07-8b20e7992179"
      },
      "source": [
        "import re\n",
        "import datetime\n",
        "import numpy as np      \n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import transformers\n",
        "from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "if torch.cuda.is_available():  \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ks7H2NTPL8yW"
      },
      "source": [
        "## pos_Tagger, Tokenizer, pretraind_embedding, Model 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjQ6rdwDMlJr"
      },
      "source": [
        "#config1 = '/content/5.Model/tokenizer_config.json'\n",
        "#config2 = '/content/5.Model/config.json'\n",
        "#tokenizer1 = ElectraTokenizer.from_pretrained('/content/5.Model/vocab.txt', cofing=config1)\n",
        "#model2 = ElectraForSequenceClassification.from_pretrained('/content/5.Model/',num_labels=2,config=config2)\n",
        "#model2.cuda()\n",
        "#not working loading model above so substitute the way below\n",
        "\n",
        "model1 = torch.load('/content/model.pt')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPXeBjyxTwrf"
      },
      "source": [
        "## 형태소 분석 + 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ys2Yu0r49to"
      },
      "source": [
        "#preprocessing text\n",
        "def preprocess(text):\n",
        "    # remove hyperlinks\n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n",
        "    text = re.sub(r'http?:\\/\\/.*[\\r\\n]*', '', text)\n",
        "    #remove some puncts (except . ! ?)\n",
        "    text=re.sub(r'[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', '',text)\n",
        "    text=\" \".join(text.split())\n",
        "    return text\n",
        "\n",
        "test['clean_text'] = test['content'].apply(preprocess)\n",
        "x_text = test['clean_text'].values\n",
        "\n",
        "#tokenizing and put data in tensor\n",
        "indices1=tokenizer.batch_encode_plus(x_text,max_length=120,add_special_tokens=True, return_attention_mask=True, padding='max_length',truncation=True)\n",
        "input_ids1=indices1[\"input_ids\"]\n",
        "attention_masks1=indices1[\"attention_mask\"]\n",
        "\n",
        "prediction_inputs1= torch.tensor(input_ids1)\n",
        "prediction_masks1 = torch.tensor(attention_masks1)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32 \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data1 = TensorDataset(prediction_inputs1, prediction_masks1)\n",
        "prediction_sampler1 = SequentialSampler(prediction_data1)\n",
        "prediction_dataloader1 = DataLoader(prediction_data1, sampler=prediction_sampler1, batch_size=batch_size)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZM1O53VojQ4"
      },
      "source": [
        "## 예측 \n",
        "\n",
        "*(/content/sample_submission.csv 필요)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vGRMJ4Boic9"
      },
      "source": [
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs1)))\n",
        "model.eval()\n",
        "predictions = []\n",
        "prediction_inputs1= torch.tensor(input_ids1) \n",
        "for batch in prediction_dataloader1:\n",
        "  batch = tuple(t.to(device) for t in batch) # Add batch to GPU\n",
        "  b_input_ids1, b_input_mask1 = batch # Unpack the inputs from our dataloader\n",
        "  with torch.no_grad(): # Telling the model not to compute or store gradients, saving memory and speeding up prediction \n",
        "      outputs1 = model(b_input_ids1, token_type_ids=None, attention_mask=b_input_mask1) # Forward pass, calculate logit predictions\n",
        "  logits1 = outputs1[0]\n",
        "  logits1 = logits1.detach().cpu().numpy() # Move logits and labels to CPU\n",
        "  predictions.append(logits1) # Store predictions and true labels\n",
        "\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "sub=pd.read_csv('/content/sample_submission.csv')\n",
        "sub=pd.DataFrame({'id':sub['id'].values.tolist(),'info':flat_predictions})\n",
        "#Ad-sentence filtering\n",
        "test['info'] = sub['info']\n",
        "train_unique_ad_sentence = train.query('info == \"1\"')['clean_text'].unique()\n",
        "test.loc[test['clean_text'].isin(train_unique_ad_sentence),['info']] = 1\n",
        "sub['info'] = test['info']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwPDIiweoujE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ba6553f-b4c0-4f5e-e135-06ff767a4a8e"
      },
      "source": [
        "print(time.time() - start)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "709.2109813690186\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoCkrceNnSn3"
      },
      "source": [
        "# 제출"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7boZTsGonU_9"
      },
      "source": [
        "sub.to_csv('/content/submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4APBU8HPgNXv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}